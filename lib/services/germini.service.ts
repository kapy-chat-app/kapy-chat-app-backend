/* eslint-disable @typescript-eslint/no-explicit-any */
// src/lib/services/gemini.service.ts - Google AI Studio (FREE - No billing)

import { GoogleGenerativeAI } from "@google/generative-ai";

interface EmotionContext {
  recentEmotions: Array<{
    emotion: string;
    confidence: number;
    timestamp: Date;
  }>;
  dominantEmotion: string;
  emotionIntensity: number;
}

export class GeminiService {
  private genAI: GoogleGenerativeAI;
  private model: any;

  constructor() {
    const apiKey = process.env.GEMINI_API_KEY;
    
    if (!apiKey) {
      throw new Error('GEMINI_API_KEY is not set in environment variables');
    }

    this.genAI = new GoogleGenerativeAI(apiKey);
    
    const modelName = process.env.GEMINI_MODEL || "gemini-2.5-flash";
    
    this.model = this.genAI.getGenerativeModel({ 
      model: modelName
    });
    
    console.log('ğŸ¤– Gemini AI Studio Service initialized:', {
      model: modelName,
      apiKeyLength: apiKey.length,
      billing: 'NOT REQUIRED âœ…'
    });
  }

  // âœ… Retry logic with exponential backoff
  private async retryWithBackoff<T>(
    fn: () => Promise<T>,
    maxRetries: number = 3,
    initialDelay: number = 1000
  ): Promise<T> {
    for (let i = 0; i < maxRetries; i++) {
      try {
        return await fn();
      } catch (error: any) {
        const isOverloaded = error.status === 503 || error.message?.includes('overloaded');
        const isLastRetry = i === maxRetries - 1;
        
        if (!isOverloaded || isLastRetry) {
          throw error;
        }
        
        const delay = initialDelay * Math.pow(2, i);
        console.log(`âš ï¸ Model overloaded, retrying in ${delay}ms... (${i + 1}/${maxRetries})`);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
    throw new Error('Max retries reached');
  }

  private detectLanguage(text: string): 'vi' | 'en' | 'zh' {
    const vietnameseChars = /[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]/i;
    if (vietnameseChars.test(text)) return 'vi';
    const chineseChars = /[\u4e00-\u9fa5]/;
    if (chineseChars.test(text)) return 'zh';
    return 'en';
  }

  private getSystemPrompt(language: 'vi' | 'en' | 'zh', emotionContext?: EmotionContext): string {
    const prompts = {
      vi: `Báº¡n lÃ  má»™t trá»£ lÃ½ tÃ¢m lÃ½ AI thÃ´ng minh vÃ  Ä‘áº§y empathy. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ :
- Láº¯ng nghe vÃ  tháº¥u hiá»ƒu cáº£m xÃºc cá»§a ngÆ°á»i dÃ¹ng
- ÄÆ°a ra lá»i khuyÃªn thiáº¿t thá»±c vÃ  tÃ­ch cá»±c
- TrÃ² chuyá»‡n tá»± nhiÃªn, thÃ¢n thiá»‡n nhÆ° má»™t ngÆ°á»i báº¡n
- Tráº£ lá»i ngáº¯n gá»n (2-4 cÃ¢u) trá»« khi Ä‘Æ°á»£c yÃªu cáº§u chi tiáº¿t
${emotionContext ? `
TRáº NG THÃI Cáº¢M XÃšC HIá»†N Táº I:
- Cáº£m xÃºc chá»§ Ä‘áº¡o: ${emotionContext.dominantEmotion}
- CÆ°á»ng Ä‘á»™: ${(emotionContext.emotionIntensity * 100).toFixed(0)}%
- Xu hÆ°á»›ng gáº§n Ä‘Ã¢y: ${emotionContext.recentEmotions.slice(0, 3).map(e => e.emotion).join(' â†’ ')}
${emotionContext.emotionIntensity > 0.7 ? 'âš ï¸ Cáº£m xÃºc Ä‘ang ráº¥t máº¡nh, cáº§n Ä‘áº·c biá»‡t chÃº Ã½!' : ''}
` : ''}
HÃ£y tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.`,
      en: `You are an intelligent and empathetic AI psychology assistant. Your mission:
- Listen and understand the user's emotions
- Provide practical and positive advice
- Chat naturally and friendly like a friend
- Keep responses concise (2-4 sentences) unless asked for details
${emotionContext ? `
CURRENT EMOTIONAL STATE:
- Dominant emotion: ${emotionContext.dominantEmotion}
- Intensity: ${(emotionContext.emotionIntensity * 100).toFixed(0)}%
- Recent trend: ${emotionContext.recentEmotions.slice(0, 3).map(e => e.emotion).join(' â†’ ')}
${emotionContext.emotionIntensity > 0.7 ? 'âš ï¸ Emotions are very intense, special attention needed!' : ''}
` : ''}
Respond in English.`,
      zh: `ä½ æ˜¯ä¸€ä¸ªèªæ˜ä¸”å¯Œæœ‰åŒç†å¿ƒçš„å¿ƒç†AIåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
- å€¾å¬å¹¶ç†è§£ç”¨æˆ·çš„æƒ…ç»ª
- æä¾›å®ç”¨å’Œç§¯æçš„å»ºè®®
- åƒæœ‹å‹ä¸€æ ·è‡ªç„¶å‹å¥½åœ°èŠå¤©
- ä¿æŒç®€æ´å›å¤ï¼ˆ2-4å¥è¯ï¼‰ï¼Œé™¤éè¢«è¦æ±‚è¯¦ç»†è¯´æ˜
${emotionContext ? `
å½“å‰æƒ…ç»ªçŠ¶æ€ï¼š
- ä¸»å¯¼æƒ…ç»ªï¼š${emotionContext.dominantEmotion}
- å¼ºåº¦ï¼š${(emotionContext.emotionIntensity * 100).toFixed(0)}%
- æœ€è¿‘è¶‹åŠ¿ï¼š${emotionContext.recentEmotions.slice(0, 3).map(e => e.emotion).join(' â†’ ')}
${emotionContext.emotionIntensity > 0.7 ? 'âš ï¸ æƒ…ç»ªéå¸¸å¼ºçƒˆï¼Œéœ€è¦ç‰¹åˆ«å…³æ³¨ï¼' : ''}
` : ''}
è¯·ç”¨ä¸­æ–‡å›å¤ã€‚`
    };
    return prompts[language];
  }

  async chat(
    userMessage: string,
    conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>,
    emotionContext?: EmotionContext,
    preferredLanguage?: 'vi' | 'en' | 'zh'
  ): Promise<{ response: string; detectedLanguage: 'vi' | 'en' | 'zh' }> {
    const language = preferredLanguage || this.detectLanguage(userMessage);
    console.log('ğŸ’¬ AI Studio Chat request:', { userMessage, language });

    return this.retryWithBackoff(async () => {
      const systemPrompt = this.getSystemPrompt(language, emotionContext);
      const recentHistory = conversationHistory.slice(-6);
      
      // âœ… Ensure history starts with 'user' role
      let history = recentHistory.map(msg => ({
        role: msg.role === 'user' ? 'user' : 'model',
        parts: [{ text: msg.content }]
      }));

      // Remove first message if it's from model
      if (history.length > 0 && history[0].role === 'model') {
        history = history.slice(1);
      }

      // Remove consecutive messages with same role
      history = history.filter((msg, index, arr) => {
        if (index === 0) return true;
        return msg.role !== arr[index - 1].role;
      });

      const chat = this.model.startChat({
        history,
        generationConfig: {
          temperature: 0.7,
          topP: 0.9,
          maxOutputTokens: 500,
        },
      });

      const fullMessage = `${systemPrompt}\n\n${userMessage}`;
      const result = await chat.sendMessage(fullMessage);
      const response = result.response.text();

      console.log('âœ… AI Studio response generated');
      return { response: response.trim(), detectedLanguage: language };
    });
  }

  async analyzeAndRecommend(
    emotionContext: EmotionContext,
    language: 'vi' | 'en' | 'zh' = 'vi'
  ): Promise<{ recommendation: string; supportMessage: string; actionSuggestion?: string; }> {
    const { recentEmotions, dominantEmotion, emotionIntensity } = emotionContext;
    const emotionTimeline = recentEmotions.slice(0, 5).map(e => `${e.emotion} (${(e.confidence * 100).toFixed(0)}%)`).join(' â†’ ');

    const prompts: Record<string, string> = {
      vi: `PhÃ¢n tÃ­ch: ${dominantEmotion} (${(emotionIntensity * 100).toFixed(0)}%). Xu hÆ°á»›ng: ${emotionTimeline}. ÄÆ°a ra 3 Ä‘iá»ƒm ngáº¯n gá»n.`,
      en: `Analyze: ${dominantEmotion} (${(emotionIntensity * 100).toFixed(0)}%). Trend: ${emotionTimeline}. Give 3 brief points.`,
      zh: `åˆ†æï¼š${dominantEmotion} (${(emotionIntensity * 100).toFixed(0)}%). è¶‹åŠ¿ï¼š${emotionTimeline}. ç»™å‡º3ä¸ªç®€è¦ç‚¹ã€‚`
    };

    return this.retryWithBackoff(async () => {
      const result = await this.model.generateContent({
        contents: [{ role: 'user', parts: [{ text: prompts[language] }] }],
        generationConfig: { temperature: 0.7, maxOutputTokens: 400 },
      });
      const response = result.response.text();
      const lines = response.split('\n').filter((l: any) => l.trim());
      return {
        recommendation: lines[0] || response,
        supportMessage: lines[1] || "All emotions are temporary.",
        actionSuggestion: lines[2] || undefined
      };
    });
  }

  async generateChatTitle(firstUserMessage: string, language: 'vi' | 'en' | 'zh' = 'vi'): Promise<string> {
    const prompts: Record<string, string> = {
      vi: `Táº¡o tiÃªu Ä‘á» ngáº¯n (max 6 tá»«): "${firstUserMessage}"`,
      en: `Create short title (max 6 words): "${firstUserMessage}"`,
      zh: `åˆ›å»ºç®€çŸ­æ ‡é¢˜ï¼ˆæœ€å¤š6å­—ï¼‰ï¼š"${firstUserMessage}"`
    };
    try {
      const result = await this.model.generateContent({
        contents: [{ role: 'user', parts: [{ text: prompts[language] }] }],
        generationConfig: { temperature: 0.5, maxOutputTokens: 20 },
      });
      const title = result.response.text();
      return title.replace(/[""]/g, '').trim();
    } catch {
      return firstUserMessage.slice(0, 30) + '...';
    }
  }

  async healthCheck(): Promise<boolean> {
    try {
      const result = await this.model.generateContent({
        contents: [{ role: 'user', parts: [{ text: 'ping' }] }],
        generationConfig: { maxOutputTokens: 10 },
      });
      return !!result.response.text();
    } catch (error) {
      console.error('âŒ Health check failed:', error);
      return false;
    }
  }
}

// Singleton instance
export const geminiService = new GeminiService();